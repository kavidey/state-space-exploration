%\documentclass[11pt]{hmcpset}
\documentclass[12pt]{article}
\usepackage[margin=1in,top=1in]{geometry}
\usepackage{graphicx}  % Required for inserting images
\usepackage{tcolorbox}  % Required for problem statement boxes
\usepackage[calc,sets,vectors,phys,misc,LinAlg,prob,mats]{shortmath}
%\usepackage[symbol]{footmisc}
\usepackage[utf8]{inputenc}
\usepackage{multicol}
\usepackage{hyperref}
\usepackage{mathtools}


\usepackage{tikz}
\newcommand*\circled[1]{\tikz[baseline=(char.base)]{
    \node[shape=circle,draw,inner sep=0.8pt] (char) {#1};}}

\renewcommand{\thefootnote}{\fnsymbol{footnote}}
\newcommand{\ang}{\mathring{\mathrm{A}}}

\newtcolorbox[]{problem}[1][]{title=#1}
\newenvironment{solution}[1][]{\def\ending{#1}}{\hfill{\ending}\bigskip}
\newenvironment{solution*}{}{}

\usepackage{enumitem}
\setlist[enumerate,1]{label=(\alph*)}

\title{SVAE LDS KL Divergence}
\author{Kavi Dey}

\begin{document}

\maketitle

\section{Starting Loss Function}
Start with the KL Divergence between two sequences of length $T$ (predicted and updated):
\begin{align}
    KL(q||p) &= \E_{q(z|x)} \left[\log \frac{q(z|x)}{p(z)} \right] \\
    &= \E_{q(z|x)} \left[ \log q(z|x) - log p(z) \right] \\
    &= \E_{q(z|x)} \left[ \log q(z_1|x) + \sum_{i=2}^T \log q(z_i | z_{i-1}, x) - \log p(z_1) - \sum_{i=2}^T \log p(z_i|z_{i-1})\right]
\end{align}
The terms outside of the summation can be separated and form their own KL divergence:
\begin{align}
    &= \underbrace{KL(q(z_1|x)||p(z_1))}_\text{Term A} + \sum_{i=2}^{T} \E_{q(z_i,z_{i-1}|x)} \left[ \underbrace{\log q(z_i | z_{i-1}, x)}_{\text{Term B}} - \underbrace{\log p(z_i| z_{i-1})}_\text{Term C} \right]
\end{align}
The KL divergence in term A can be calculated in closed form, terms B and C are calculated below. Term C is simpler than term B so we will calculate it first.

\section{Kalman Filter Equations} \label{sec:kalman_filter}
In \textit{Bayesian Filtering and Smoothing} (S\"arkk\"a and Svensson) describe a kalman filter as follows:
\begin{align}
    z_i &= Az_{i-1} b + q \\
    x_i &= H z_i + r
\end{align}
Where $z_i$ is the state, $A$ is the transition matrix, and $b$ is a constant input. $x_i$ is the measurement and $H$ is the measurement matrix. $q \sim \mathcal{N}(0, Q)$ is the process noise
and $r \sim \mathcal{N}(0, R)$ is the measurement noise, and $z_0 \sim \mathcal{N}(\mu_0, P_0)$ is the prior.  \\
\\
Then we can write the prediction and update equations as follows:\\
Prediction Step Equations:
\begin{align}
    z_{i|i-1} &= \mA z_{i-1|i-1} + \vb \\
    P_{i|i-1} &= \mA P_{i-1|i-1} + \mA^T + Q \\
\end{align}
Update Step Equations
\begin{align}
    K_i  &= P_{i|i-1}H^T(HP_{i|i-1}H^T + R)^{-1} \qquad \text{(Kalman Gain)} \\
    z_{i|i} &= z_{i|i-1} + K_i (x_i - Hz_{i|i-1})  \\
    P_{i|i} &= P_{i|i-1} - K_i H P_{i|i-1} = (I-K_iH) P_{i|i-1}
\end{align}
It will also be useful to write out the covariance of the joint distribution between $z_{i|i-1}$ and $z_{i-1|i-1}$ as well as $z_{i|i}$ and $z{i|i-1}$.
Following \textit{Probabilistic Machine Learning: Advanced Topics} by Kevin Murphy (8.40, 8.45), we can write the covariance as:
\begin{align}
    \Sigma_{z_{i|i-1},z_{i-1|i-1}} & = \begin{pmatrix}
        \Sigma_{z_{i|i-1}z_{i|i}} & \Sigma_{z_{i|i-1}z_{i-1|i-1}} \\
        \Sigma_{z_{i-1|i-1}z_{i|i-1}} & \Sigma_{z_{i-1|i-1}z_{i-1|i-1}} \\
    \end{pmatrix} 
    = \begin{pmatrix}
        P_{i-1|i-1} & P_{i-1|i-1} A^T \\
        A P_{i-1|i-1} & A P_{i-1|i-1} A^T + Q
    \end{pmatrix} \\
    \Sigma_{z_{i|i},z_{i|i-1}} & = \begin{pmatrix}
        \Sigma_{z_{i|i}z_{i|i}} & \Sigma_{z_{i|i}z_{i|i-1}} \\
        \Sigma_{z_{i|i-1}z_{i|i}} & \Sigma_{z_{i|i-1}z_{i|i-1}} \\
    \end{pmatrix} 
    = \begin{pmatrix}
        P_{i|i-1} & P_{i|i-1}H^T \\
        H^TP_{i|i-1} & HP_{i|i-1}^{-1}H^T+R \\
    \end{pmatrix} 
\end{align}

\section{Calculating Term C}
Lets start with term C from section 1. From the kalman filter update step $\mu_{i|i-1} = \mA z_{i-1|i-1} + \vb$ and $P_{i|i-1} = \mA P_{i-1} \mA^T + Q$. Additionally we use the shorthand $\E_q = \E_{q(z_i, z_{i-1} | x)}$.
\begin{align}
    \hspace*{-2cm} \E_q \left[ \log p(z_i|z_{i-1})\right] &= -\frac{1}{2} \E_q \left[ k \log (2\pi) + \log \det P_i \right] - \frac{1}{2} \E_q \left[ (z_{i|i-1} - \mu_{i|i-1})^T P_i (z_{i|i-1} - \mu_{i|i-1}) \right] \\
    &= -\frac{1}{2} \left[ k \log (2\pi) + \log \det P_i \right] - \frac{1}{2} \E_q \left[ (z_{i|i-1} - \mu_{i|i-1})^T P_i (z_{i|i-1} - \mu_{i|i-1}) \right] \\
    &= -\frac{1}{2} \left[ k \log (2\pi) + \log \det P_i \right] \nonumber \\
    & \quad - \frac{1}{2} \E_q \Big[ \underbrace{z_{i|i-1}^T P_{i|i-1}^{-1} z_{i|i-1}}_{\circled{1}} - \underbrace{z_{i|i-1}^T P_{i|i-1}^{-1}\mA z_{i-1|i-1}}_{\circled{2}} - \underbrace{z_{i|i-1}^T P_{i|i-1}^{-1}\vb}_{\circled{3}} - \underbrace{\mA^Tz_{i-1|i-1}^T P_{i|i-1}^{-1} z_{i|i-1}}_{\circled{4}}  \nonumber \\
    & \quad + \underbrace{\mA^Tz_{i-1|i-1}^T P_{i|i-1}^{-1}\mA z_{i-1|i-1}}_{\circled{5}} + \underbrace{\mA^Tz_{i-1|i-1}^T P_{i}^{-1}\vb}_{\circled{6}} - \underbrace{\vb^T P_{i}^{-1} z_{i|i-1}}_{\circled{7}} + \underbrace{\vb^T P_{i}^{-1} \mA z_{i-1|i-1}}_{\circled{8}} + \underbrace{\vb^T P_{i}^{-1} \vb}_{\circled{9}} \Big]
\end{align}
Term $\circled{9}$ is constant and can be pulled out of the expectation,
all other terms can be calculated with expectation identities from appendix \ref{sec:expectation_identities}.
\begin{alignat*}{3}
    {\circled{1}:} \quad && \E_q[z_{i|i-1}^T P_{i|i-1}^{-1} z_{i|i-1}] &= \tr [P_{i|i-1}^{-1}(\mu_{i|i-1}\mu_{i|i-1}^T+P_{i|i-1}^{-1})] \\
    {\circled{2}:} \quad && \E_q[z_{i|i-1}^T P_{i|i-1}^{-1}\mA z_{i-1|i-1}] &= 1\\
    {\circled{3}:} \quad && \E_q[z_{i|i-1}^T P_{i|i-1}^{-1}\vb] &= 1\\
    {\circled{4}:} \quad && \E_q[\mA^Tz_{i-1|i-1}^T P_{i|i-1}^{-1} z_{i|i-1}] &= 1\\
    {\circled{5}:} \quad && \E_q[\mA^Tz_{i-1|i-1}^T P_{i|i-1}^{-1}\mA z_{i-1|i-1}] &= 1\\
    {\circled{6}:} \quad && \E_q[\mA^Tz_{i-1|i-1}^T P_{i}^{-1}\vb] &= 1\\
    {\circled{7}:} \quad && \E_q[\vb^T P_{i}^{-1} z_{i|i-1}] &= 1\\
    {\circled{8}:} \quad && \E_q[\vb^T P_{i}^{-1} \mA z_{i-1|i-1}] &= 1\\ 
    {\circled{9}:} \quad && \E_q[\vb^T P_{i}^{-1} \vb] &= \vb^T P_{i}^{-1} \vb
\end{alignat*}

\section{Calculating Term B}
The process for calculating term B starts the same way as term A. $q(z_i|z_{i-1})$ represents the update step of the kalman filter, so the equations for $\mu_i$ and $\Sigma_i$ are more complicated.
\begin{align}
    \E_q \left[ \log p(z_i|z_{i-1})\right] &= -\frac{1}{2} \E_q \left[ k \log (2\pi) + \log \det P_i \right] - \frac{1}{2} \E_q \left[ (z_i - \mu_i)^T P_i (z_i - \mu_i) \right] \label{eqn:b_unexpanded}
\end{align}
Using traditional kalman filter from section \ref{sec:kalman_filter} we get:
\begin{align}
    \mu_i & = \mA z_{i-1|i-1} + \vb + K_i (x_i - H(\mA z_{i-1|i-1} + \vb)) \\
    P_{i|i} &= P_{i|i-1} - K_i H P_{i|i-1} = (I-K_iH) P_{i|i-1}
\end{align}
$P_{i|i}$ is constant with respect to $z$, so we don't need to expand the full equation.
It will be helpful to simplify the equation for $\mu_i$ as follows:
\begin{align}
    \mu_i & = \mA z_{i-1|i-1} + \vb + K_i (x_i - H(\mA z_{i-1|i-1} + \vb)) \\
    &= \mA z_{i-1|i-1} + \vb + K_i x_i - K_i H \mA z_{i-1|i-1} - K_i H \vb \\ 
    &= \mA z_{i-1|i-1} - K_i H \mA z_{i-1|i-1} + \underbrace{\vb - K_i H \vb + K_i x_i}_{\text{constant} \ C}
\end{align}
Now we can expand equation \ref{eqn:b_unexpanded} into:
\begin{align}
    \hspace*{-2cm} \E_q \left[ \log p(z_i|z_{i-1})\right] &= -\frac{1}{2} \left[ k \log (2\pi) + \log \det P_i \right] \nonumber \\
    &\quad - \frac{1}{2} \E_q \left[ (z_{i|i} - \mu_i)^T P_i (z_{i|i} - \mu_i) \right] \\
    &= -\frac{1}{2} \left[ k \log (2\pi) + \log \det P_i \right] \nonumber \\
    &\quad - \frac{1}{2} \E_q \left[ (z_{i|i} - \mA z_{i-1|i-1} + K_i H \mA z_{i-1|i-1} - C)^T P_i (z_{i|i} - \mA z_{i-1|i-1} + K_i H \mA z_{i-1|i-1} - C) \right] \\
\end{align}
\begin{align}
    &= -\frac{1}{2} \left[ k \log (2\pi) + \log \det P_i \right] \nonumber \\
    &\quad - \frac{1}{2} \E_q \Big[ \underbrace{z_{i|i}^TP^{-1}_{i|i}z_{i|i}}_{\circled{1}} - \underbrace{z_{i|i}^TP^{-1}_{i|i}Az_{i|i-1}}_{\circled{2}} + \underbrace{z_{i|i}^TP^{-1}_{i|i}K_iHAz_{i|i-1}}_{\circled{3}} - \underbrace{z_{i|i}^TP^{-1}_{i|i}C}_{\circled{4}} \nonumber \\
    &\quad - \underbrace{A^Tz^T_{i|i-1}P^{-1}_{i|i}z_{i|i}}_{\circled{5}} + \underbrace{A^Tz^T_{i|i-1}P^{-1}_{i|i}Az_{i|i-1}}_{\circled{6}} - \underbrace{A^Tz^T_{i|i}P^{-1}_{i|i}K_iHAz_{i|i-1}}_{\circled{7}} + \underbrace{A^Tz^T_{i|i}P^{-1}_{i|i}C}_{\circled{8}} \nonumber \\
    &\quad + \underbrace{K_i^TH^TA^Tz^T_{i|i-1}P^{-1}_{i|i}z_{i|i}}_{\circled{9}} - \underbrace{K_i^TH^TA^Tz^T_{i|i-1}P^{-1}_{i|i}Az_{i|i-1}}_{\circled{10}} \nonumber \\
    &\qquad \qquad + \underbrace{K_i^TH^TA^Tz^T_{i|i-1}P^{-1}_{i|i}KHAz_{i|i-1}}_{\circled{11}} - \underbrace{K_i^TH^TA^Tz^T_{i|i-1}P^{-1}_{i|i}C}_{\circled{12}} \nonumber \\
    &\quad - \underbrace{C^TP^{-1}_{i|i}z_{i|i}}_{\circled{13}} + \underbrace{C^TP^{-1}_{i|i}Az_{i|i-1}}_{\circled{14}} - \underbrace{C^TP^{-1}_{i|i}K_iHAz_{i|i-1}}_{\circled{15}} + \underbrace{C^TP^{-1}_{i|i}C}_{\circled{16}} \Big]
\end{align}
Term $\circled{16}$ is constant and can be pulled out of the expectation,
all other terms can be calculated with expectation identities from appendix \ref{sec:expectation_identities}.
{\allowdisplaybreaks
\begin{align}
    {\circled{1}:} \quad && \E_q[z_{i|i}^TP^{-1}_{i|i}z_{i|i}] &= \tr [P^{-1}_{i|i}(\mu_{i|i}\mu_{i|i}^T + P^{-1}_{i|i})]\\
    {\circled{2}:} \quad && \E_q[z_{i|i}^TP^{-1}_{i|i}Az_{i|i-1}] &= 1\\
    {\circled{3}:} \quad && \E_q[z_{i|i}^TP^{-1}_{i|i}K_iHAz_{i|i-1}] &= 1\\
    {\circled{4}:} \quad && \E_q[z_{i|i}^TP^{-1}_{i|i}C] &= \mu_{i|i}^TP^{-1}_{i|i}C\\
    {\circled{5}:} \quad && \E_q[A^Tz^T_{i|i-1}P^{-1}_{i|i}z_{i|i}] &= 1\\
    {\circled{6}:} \quad && \E_q[A^Tz^T_{i|i-1}P^{-1}_{i|i}Az_{i|i-1}] &= 1\\
    {\circled{7}:} \quad && \E_q[A^Tz^T_{i|i}P^{-1}_{i|i}K_iHAz_{i|i-1}] &= 1\\
    {\circled{8}:} \quad && \E_q[A^Tz^T_{i|i}P^{-1}_{i|i}C] &= 1\\ 
    {\circled{9}:} \quad && \E_q[K_i^TH^TA^Tz^T_{i|i-1}P^{-1}_{i|i}z_{i|i}] &= 1\\
    {\circled{10}:} \quad && \E_q[K_i^TH^TA^Tz^T_{i|i-1}P^{-1}_{i|i}Az_{i|i-1}] &= 1\\
    {\circled{11}:} \quad && \E_q[K_i^TH^TA^Tz^T_{i|i-1}P^{-1}_{i|i}KHAz_{i|i-1}] &= 1\\
    {\circled{12}:} \quad && \E_q[K_i^TH^TA^Tz^T_{i|i-1}P^{-1}_{i|i}C] &= 1\\
    {\circled{13}:} \quad && \E_q[C^TP^{-1}_{i|i}z_{i|i}] &= 1\\ 
    {\circled{14}:} \quad && \E_q[C^TP^{-1}_{i|i}Az_{i|i-1}] &= 1\\
    {\circled{15}:} \quad && \E_q[C^TP^{-1}_{i|i}K_iHAz_{i|i-1}] &= 1 \\
    {\circled{16}:} \quad && \E_q[C^TP^{-1}_{i|i}C] &= C^TP^{-1}_{i|i}C
\end{align}
}

\appendix
\section{Expectation Identities} \label{sec:expectation_identities}
\begin{align}
    \E_{z \sim \mathcal{N}(\mu, \Sigma)} [z] &= \mu \\
    \E_{z \sim \mathcal{N}(\mu, \Sigma)} [zz^T] &= \mu\mu^T + \Sigma \\
    % second moment of multivariate normal is mu mu^T + sigma (Pattern Recognition and Machine Learning, 83)
    % \E_{{z \sim \mathcal{N}(\mu, \Sigma)}} [z^T\mA z] &= \tr [A(\mu\mu^T + \Sigma)]  \\
    \E_{y \sim \mathcal{N}(\mu_y, \Sigma_y), z \sim \mathcal{N}(\mu_z, \Sigma_z)} [zy^T] &= \mu_z \mu_y^T + \Sigma_{zy}
    % \E_{y \sim \mathcal{N}(\mu_y, \Sigma_y), z \sim \mathcal{N}(\mu_z, \Sigma_z)} [y^T\mA z] &= \tr [A\mu\mu^T]  \\
\end{align}
\section{Matrix Identities} \label{sec:matrix_identities}
The following matrix identities are copied from Max Welling's wonderful notes on Kalman Filters (\href{http://www.stat.columbia.edu/~liam/teaching/neurostat-spr12/papers/hmm/KF-welling-notes.pdf}{stat.columbia.edu/~liam/teaching/neurostat-spr12/papers/hmm/KF-welling-notes.pdf})
\begin{align}
    \va^T \mA \vb &= \tr [\mA \vb \va^T] \\
    \tr [\mA \mB] &= \tr [\mB \mA] \\
    \log \det [\mA] &= - \log \det [\mA^{-1}]
\end{align}

\end{document}